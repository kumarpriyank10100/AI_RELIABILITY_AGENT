{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic AI: CSTR Reactor Safety System\n",
        "\n",
        "### üî¨ Chemical Engineering meets Artificial Intelligence\n",
        "\n",
        "This notebook demonstrates a **Physics-Informed Agentic System**. It combines classical Machine Learning with Generative AI to manage a Continuous Stirred-Tank Reactor (CSTR).\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. **Fault Detection:** A Random Forest model trained on synthetic reactor data.\n",
        "2. **Reasoning Agent:** Google Gemini 1.5 Flash interpreting Standard Operating Procedures (SOPs).\n",
        "3. **Safety Interlock:** A hard-coded deterministic layer that overrides AI suggestions if they violate physical safety constraints.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uEd9xhMG0b1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q -U google-generativeai gradio pandas scikit-learn"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "9NhilhQG0b10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Physics-Informed ML Training\n",
        "\n",
        "In this section, we generate synthetic data representing reactor temperatures () and pressures (). We train a `RandomForestClassifier` to identify the \"Danger Zone.\"\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cGC2IeuF0b13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "\n",
        "def train_model():\n",
        "    # Synthetic Data Generation\n",
        "    np.random.seed(42)\n",
        "    temp = np.random.uniform(300, 500, 2000)\n",
        "    press = np.random.uniform(50, 250, 2000)\n",
        "    flow = np.random.uniform(10, 100, 2000)\n",
        "\n",
        "    # Logic: Fault if Temp > 380 OR Pressure > 140\n",
        "    target = ((temp > 380) | (press > 140)).astype(int)\n",
        "\n",
        "    df = pd.DataFrame({'temp': temp, 'press': press, 'flow': flow, 'target': target})\n",
        "    model = RandomForestClassifier(n_estimators=100)\n",
        "    model.fit(df[['temp', 'press', 'flow']], df['target'])\n",
        "    return model\n",
        "\n",
        "clf = train_model()\n",
        "print(\"‚úÖ Model trained successfully on synthetic reactor data.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model trained successfully on synthetic reactor data.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pB5bC7T0b14",
        "outputId": "a251863c-27c7-4d94-e550-e1423c60812c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. AI Setup & Knowledge Base\n",
        "\n",
        "We configure the Gemini API and define the **Standard Operating Procedures (SOPs)** that the AI agent will use to make decisions.\n",
        "\n",
        "> **Note:** Replace the placeholder API key with your own from Google AI Studio.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A1w7U_a10b15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP CONFIGURATION\n",
        "# Best practice: Use Colab Secrets (Left sidebar > Key icon) to store API_KEY\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "    API_KEY = \"YOUR_KEY_HERE\" # Fallback\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "model_gemini = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "sops = {\n",
        "    \"High_Heat_Fault\": \"CRITICAL: Exothermic runaway suspected. ACTION: Increase coolant flow to max. If pressure > 150psi, open emergency vent V-301.\",\n",
        "    \"Normal_Ops\": \"System stable. Maintain current setpoints.\"\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "9DLy_Qp30b15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Core Logic & Safety Interlock\n",
        "\n",
        "This is the heart of the \"Agentic\" system.\n",
        "\n",
        "* **Safety Interlock:** If the AI hallucinates and suggests closing a vent during high pressure, this function forces a rejection.\n",
        "* **Process Monitor:** Coordinates the ML prediction, Gemini reasoning, and safety check.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FWENpvrx0b15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safety_interlock(temp, press, ai_advice):\n",
        "    \"\"\"Deterministic safety guardrail that overrides AI responses\"\"\"\n",
        "    if press > 150 and any(word in ai_advice.lower() for word in [\"close\", \"shut\"]) and \"vent\" in ai_advice.lower():\n",
        "        return \"‚ö†Ô∏è CRITICAL REJECTION: AI suggested closing vent during high pressure. Interlock forced VENT OPEN.\"\n",
        "    return \"‚úÖ SAFETY CLEARED\"\n",
        "\n",
        "def process_monitor(temp, press, flow):\n",
        "    # Step A: ML Prediction\n",
        "    input_df = pd.DataFrame([[temp, press, flow]], columns=['temp', 'press', 'flow'])\n",
        "    prediction = clf.predict(input_df)[0]\n",
        "\n",
        "    # Manual Sensitivity Override for Demo\n",
        "    if temp > 385 or press > 145:\n",
        "        status = \"High_Heat_Fault\"\n",
        "    else:\n",
        "        status = \"High_Heat_Fault\" if prediction == 1 else \"Normal_Ops\"\n",
        "\n",
        "    # Step B: Agentic Reasoning (Gemini)\n",
        "    prompt = f\"\"\"\n",
        "    You are a Lead Process Engineer.\n",
        "    Current Sensors: Temp {temp}K, Pressure {press}psi, Flow {flow}L/min.\n",
        "    ML Detection: {status}\n",
        "    Official SOP: {sops[status]}\n",
        "\n",
        "    Instruction: Give one clear, 1-sentence engineering command to the operator.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model_gemini.generate_content(prompt)\n",
        "        ai_response = response.text\n",
        "    except Exception as e:\n",
        "        if status == \"High_Heat_Fault\":\n",
        "            ai_response = \"EMERGENCY: High heat detected. Increase coolant flow. (Local Override)\"\n",
        "        else:\n",
        "            ai_response = \"All systems normal. Continue monitoring. (Local Override)\"\n",
        "\n",
        "    # Step C: Safety Verification\n",
        "    safety = safety_interlock(temp, press, ai_response)\n",
        "\n",
        "    return status.replace(\"_\", \" \"), ai_response, safety"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "iN7aCF7y0b16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Live Dashboard\n",
        "\n",
        "Run this cell to launch the interactive Gradio interface directly inside your notebook.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XmxeyWbs0b17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=process_monitor,\n",
        "    inputs=[\n",
        "        gr.Slider(300, 500, value=350, label=\"Reactor Temp (K)\"),\n",
        "        gr.Slider(50, 250, value=100, label=\"Pressure (psi)\"),\n",
        "        gr.Slider(10, 100, value=50, label=\"Inlet Flow (L/min)\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Label(label=\"System Status\"),\n",
        "        gr.Textbox(label=\"AI Engineer Instruction\"),\n",
        "        gr.Textbox(label=\"Safety Interlock Status\")\n",
        "    ],\n",
        "    title=\"‚ò¢Ô∏è CSTR Reactor Safety Dashboard\",\n",
        "    description=\"Agentic AI system combining Random Forest (Fault Detection) and Gemini LLM (SOP Reasoning) with a hard-coded Safety Interlock.\",\n",
        "    theme=\"glass\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://28501e0eaf98c92fec.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://28501e0eaf98c92fec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 759.82ms\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "FpwHPqAT0b17",
        "outputId": "1c9e1d64-6219-4942-d419-d1a2d881e2a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Would you like me to add a **Data Visualization** block to show the training data's \"Danger Zone\" before launching the UI?\n",
        "\n",
        "[Integrating Gemini AI with Google Colab](https://www.youtube.com/watch?v=PytsxNfR8GI)\n",
        "This video explains how to set up your environment and use API keys securely within the Google Colab ecosystem for data science projects."
      ],
      "metadata": {
        "id": "VGFctNtO0b17"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}